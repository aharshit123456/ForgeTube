# -*- coding: utf-8 -*-
"""llasatts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ABGpUCsolDvreFVq1AjX-93ehvJ1yq00
"""

!pip install xcodec2==0.1.3

!pip install numpy==1.26.4
!pip uninstall -y torch torchvision
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

from huggingface_hub import snapshot_download

# snapshot download faster than normal download since its multiworker
snapshot_download(repo_id="srinivasbilla/llasa-3b", local_dir="/content/srinivasbilla/llasa-3b")
snapshot_download(repo_id="srinivasbilla/xcodec2", local_dir="/content/srinivasbilla/xcodec2")
snapshot_download(repo_id="openai/whisper-large-v3-turbo", local_dir="/content/openai/whisper-large-v3-turbo")

!pip install bitsandbytes -U

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
import torch
import soundfile as sf
from xcodec2.modeling_xcodec2 import XCodec2Model
import torchaudio

quantization_config = BitsAndBytesConfig(load_in_8bit=True)

llasa_3b ='/content/srinivasbilla/llasa-3b'

# be patient this takes a couple mins...
tokenizer = AutoTokenizer.from_pretrained(llasa_3b)

model = AutoModelForCausalLM.from_pretrained(
    llasa_3b,
    trust_remote_code=True,
    device_map='auto',
    quantization_config=quantization_config,
    low_cpu_mem_usage=True
)

model_path = "/content/srinivasbilla/xcodec2"

Codec_model = XCodec2Model.from_pretrained(model_path, low_cpu_mem_usage=True)
Codec_model.eval().cuda()

whisper_turbo_pipe = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-large-v3-turbo",
    torch_dtype=torch.float16,
     device='cuda',
 )

def ids_to_speech_tokens(speech_ids):

    speech_tokens_str = []
    for speech_id in speech_ids:
        speech_tokens_str.append(f"<|s_{speech_id}|>")
    return speech_tokens_str

def extract_speech_ids(speech_tokens_str):

    speech_ids = []
    for token_str in speech_tokens_str:
        if token_str.startswith('<|s_') and token_str.endswith('|>'):
            num_str = token_str[4:-2]

            num = int(num_str)
            speech_ids.append(num)
        else:
            print(f"Unexpected token: {token_str}")
    return speech_ids

def infer(sample_audio_path, target_text, prompt_text=None):
    waveform, sample_rate = torchaudio.load(sample_audio_path)
    if len(waveform[0])/sample_rate > 15:
        print("Trimming audio to first 15secs.")
        waveform = waveform[:, :sample_rate*15]

    # Check if the audio is stereo (i.e., has more than one channel)
    if waveform.size(0) > 1:
        # Convert stereo to mono by averaging the channels
        waveform_mono = torch.mean(waveform, dim=0, keepdim=True)
    else:
        # If already mono, just use the original waveform
        waveform_mono = waveform

    prompt_wav = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform_mono)

    if prompt_text is None:
        prompt_text = whisper_turbo_pipe(prompt_wav[0].numpy())['text'].strip()

    print(f"Prompt: {prompt_text}")
    print(f"Target: {target_text}")

    if len(target_text) == 0:
        return None
    elif len(target_text) > 500:
        print("Text is too long. Please keep it under 300 characters.")
        target_text = target_text[:700]

    input_text = prompt_text + ' ' + target_text

    #TTS start!
    with torch.no_grad():
        # Encode the prompt wav
        vq_code_prompt = Codec_model.encode_code(input_waveform=prompt_wav)

        vq_code_prompt = vq_code_prompt[0,0,:]
        # Convert int 12345 to token <|s_12345|>
        speech_ids_prefix = ids_to_speech_tokens(vq_code_prompt)

        formatted_text = f"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>"

        speed = "high"
        emotion = "angry"
        # Tokenize the text and the speech prefix
        chat = [
            {"role": "user", "content": f"Convert the text to speech:" + formatted_text},
            {"role": "assistant", "content": f"Having {speed} audio speed and {emotion} emotion <|SPEECH_GENERATION_START|>" + ''.join(speech_ids_prefix)},
            # {"role": "system", "content": f""}
        ]

        input_ids = tokenizer.apply_chat_template(
            chat,
            tokenize=True,
            return_tensors='pt',
            continue_final_message=True
        )
        input_ids = input_ids.to('cuda')
        speech_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')

        # Generate the speech autoregressively
        outputs = model.generate(
            input_ids,
            max_length=2048,  # We trained our model with a max length of 2048
            eos_token_id= speech_end_id ,
            do_sample=True,
            top_p=1,
            temperature=0.8
        )
        # Extract the speech tokens
        generated_ids = outputs[0][input_ids.shape[1]-len(speech_ids_prefix):-1]

        speech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

        # Convert  token <|s_23456|> to int 23456
        speech_tokens = extract_speech_ids(speech_tokens)

        speech_tokens = torch.tensor(speech_tokens).cuda().unsqueeze(0).unsqueeze(0)

        # Decode the speech tokens to speech waveform
        gen_wav = Codec_model.decode_code(speech_tokens)

        # if only need the generated part
        gen_wav = gen_wav[:,:,prompt_wav.shape[1]:]



    return gen_wav[0, 0, :].cpu().numpy()

"""#Kaggle

##OP2-> emotion- happy, temp-high
"""

# samples from https://huggingface.co/datasets/leafspark/openai-voices?row=12

# you must give prompt text if you dont load whisper
# prompt_text = "Last September, I received an offer from Sam Altman, who wanted to hire me to voice the current ChatGPT 4.0 system. He told me that he felt that by my voicing the system, I could bridge the gap between tech companies and creatives,"
prompt_text = "Kids are tucking by the door"
target_text = "The door is closing fast"
output_audio = infer('/content/op2.wav', target_text , prompt_text=prompt_text)

from IPython.display import Audio
Audio(output_audio, rate=16000)

import soundfile as sf

sf.write('output_audio2.wav', output_audio, 16000)
from google.colab import files
files.download('output_audio2.wav')

"""##OP3-> emotion- happy, temp- high"""

# samples from https://huggingface.co/datasets/leafspark/openai-voices?row=12

# you must give prompt text if you dont load whisper
# prompt_text = "Last September, I received an offer from Sam Altman, who wanted to hire me to voice the current ChatGPT 4.0 system. He told me that he felt that by my voicing the system, I could bridge the gap between tech companies and creatives,"
prompt_text = "Dogs are sitting by the door"
target_text = "Sitting is an art form"
output_audio = infer('/content/op3.wav', target_text , prompt_text=prompt_text)

from IPython.display import Audio
Audio(output_audio, rate=16000)

import soundfile as sf

sf.write('output_audio2.wav', output_audio, 16000)
from google.colab import files
files.download('output_audio2.wav')

"""##OP4- emotion: sad, tempo: low"""

# samples from https://huggingface.co/datasets/leafspark/openai-voices?row=12

# you must give prompt text if you dont load whisper
# prompt_text = "Last September, I received an offer from Sam Altman, who wanted to hire me to voice the current ChatGPT 4.0 system. He told me that he felt that by my voicing the system, I could bridge the gap between tech companies and creatives,"
prompt_text = "Kids are talking by the door"
target_text = "All doors are made of wood in this building!!"
output_audio = infer('/content/op4.wav', target_text , prompt_text=prompt_text)

from IPython.display import Audio
Audio(output_audio, rate=16000)

import soundfile as sf

sf.write('output_audio2.wav', output_audio, 16000)
from google.colab import files
files.download('output_audio2.wav')

"""#Kokoru

###OP5-> emotion: happy, tempo- high, accent: british
"""

# samples from https://huggingface.co/datasets/leafspark/openai-voices?row=12

# you must give prompt text if you dont load whisper
# prompt_text = "Last September, I received an offer from Sam Altman, who wanted to hire me to voice the current ChatGPT 4.0 system. He told me that he felt that by my voicing the system, I could bridge the gap between tech companies and creatives,"
prompt_text = "Sinners are judging sinners for sinning differently. "
target_text = "Often the best minds tend to lose objectivity in pursuit of excellence!"
output_audio = infer('/content/op5.wav', target_text , prompt_text=prompt_text)

from IPython.display import Audio
Audio(output_audio, rate=16000)

"""##Op5-> emotion: angry, tempo: high, accent: british"""

# samples from https://huggingface.co/datasets/leafspark/openai-voices?row=12

# you must give prompt text if you dont load whisper
# prompt_text = "Last September, I received an offer from Sam Altman, who wanted to hire me to voice the current ChatGPT 4.0 system. He told me that he felt that by my voicing the system, I could bridge the gap between tech companies and creatives,"
prompt_text = "Sinners are judging sinners for sinning differently. "
target_text = "Often the best minds tend to lose objectivity in pursuit of excellence!"
output_audio = infer('/content/op5.wav', target_text , prompt_text=prompt_text)

from IPython.display import Audio
Audio(output_audio, rate=16000)

"""##OP1-> large dataset; emotion: angry, tempo: high"""

# samples from https://huggingface.co/datasets/leafspark/openai-voices?row=12

# you must give prompt text if you dont load whisper
# prompt_text = "Last September, I received an offer from Sam Altman, who wanted to hire me to voice the current ChatGPT 4.0 system. He told me that he felt that by my voicing the system, I could bridge the gap between tech companies and creatives,"
prompt_text = "To the hill and down we go losing our breadth as we plump splashing into the water our body wheels take paddlestroke and carry us laughing over and up the opposite bank to the track their and the incenious course we laughed too quick says father that friend whom we laughed dropped that fork on the rail i see him behind that boulder. We leave the narrow guage track at its terminus without stopping and have no other special accident in this facility. the sun has changed frost and rose heus the higher snow peaks. seria navada snowy in its most interesting locality is around having come the narrow guage railroad two largest and the oldest mining city."
target_text = "AI is the new technology. We are very excited to move ahead and see the innovations! Although their is a lot of dilemna in the ethics of this technology? Lets hope everything is good."
output_audio = infer('/content/op1.wav', target_text , prompt_text=prompt_text)

from IPython.display import Audio
Audio(output_audio, rate=16000)